{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsSdYm6TgE4btO3TGC1H9V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuilhermeN391/Heart_Disease_App/blob/main/Heart_Disease_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importação das Bibliotecas necessárias**\n",
        "\n",
        "Bibliotecas utilizadas ao longo do desenvolvimento do modelo e armazenamento dos seus resultados."
      ],
      "metadata": {
        "id": "lGhsAEGVyjna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "!pip install ydata-profiling\n",
        "!pip install pytest pytest-sugar"
      ],
      "metadata": {
        "id": "JcYmAoyQyjG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ydata_profiling import ProfileReport\n",
        "import tempfile\n",
        "import os\n",
        "import datetime\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc, roc_auc_score\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.impute import SimpleImputer\n",
        "import glob\n",
        "import os\n",
        "import wandb\n",
        "import joblib\n",
        "import logging"
      ],
      "metadata": {
        "id": "7sYd2vhryvWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Análise Exploratória dos Dados (EDA)**\n",
        "Compreender a distribuição dos dados, se há classes\n",
        "desbalanceadas, estatística das variáveis.\n",
        "\n",
        "\n",
        "\n",
        "**O projeto escolhido foi o Heart Disaese App do Github user** *maxim-eyengue*\n",
        "\n",
        "[Link do repositório](https://github.com/maxim-eyengue/Heart-Disease-App)\n",
        "\n"
      ],
      "metadata": {
        "id": "RAHfNaDS-k1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Carregamento dos dados**\n",
        "\n",
        "Armazenamento no wandb no projeto \"Heart_Disease\""
      ],
      "metadata": {
        "id": "UDY_LR9ZLe4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features Description:\n",
        "*   **age**: age of the patient [years: Numeric]\n",
        "*   **sex**: gender of the patient [1: Male, 0: Female]\n",
        "*   **cp**: chest pain type [0: Typical Angina, 1: Atypical Angina, 2: Non-Anginal Pain, 3: Asymptomatic]\n",
        "*   **trestbps**: resting blood pressure [mm Hg: Numeric]\n",
        "*   **chol**: serum cholesterol level [mg/dl: Numeric]\n",
        "*   **fbs**: fasting blood sugar [1: if fasting blood sugar > 120 mg/dl, 0: otherwise]\n",
        "*   **restecg**: resting electrocardiographic results [0: Normal, 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), 2: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n",
        "*   **thalach**: maximum heart rate achieved [Numeric value between 60 and 202]\n",
        "*   **exang**: exercise-induced angina [1: Yes, 0: No]\n",
        "*   **oldpeak**: ST depression induced by exercise relative to rest [Numeric value measured in depression]\n",
        "*   **slope**: slope of the peak exercise ST segment [0: Upsloping, 1: Flat, 2: Downsloping]\n",
        "*   **ca**: number (0-3) of major vessels (arteries, veins, and capillaries) colored by fluoroscopy [0, 1, 2, 3]\n",
        "*   **thal**: Thalassemia types [1: Normal, 2: Fixed defect, 3: Reversible defect]\n",
        "*   **target**: outcome variable for heart attack risk [1: disease or more chance of heart attack, 0: normal or less chance of heart attack]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rcD1RuhoUk7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colunas utilizadas\n",
        "columns = ['age', 'sex', 'cp', 'trestbps', 'chol',\n",
        "           'fbs', 'restecg', 'thalachh', 'exang',\n",
        "           'oldpeak','slope','ca', 'thal', 'target']\n",
        "# Importação do Dataset\n",
        "income = pd.read_csv(\"https://raw.githubusercontent.com/maxim-eyengue/Heart-Disease-App/03add8ca4ceb193042893c3a1bffe9c65b5f7cbd/data/raw_merged_heart_dataset.csv\",\n",
        "                   header=None,\n",
        "                   names=columns,\n",
        "                   skiprows=1 # Ignora a primeira linha, que não possui dados\n",
        "                   )\n",
        "income.head()"
      ],
      "metadata": {
        "id": "-3tZt2XfFBz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Informações referentes as *features* do Dataset importado"
      ],
      "metadata": {
        "id": "vJDj4m1p1E4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "income.info()"
      ],
      "metadata": {
        "id": "BiIps875yZRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Conversões de variáveis**\n",
        "\n",
        "Para tratar e codificar os dados recebidos do dataset, envolvendo correção de dados inconsistentes e conversão de Dtype nos casos necessários."
      ],
      "metadata": {
        "id": "0D26FMaMfjC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Map sex values\n",
        "income[\"sex\"] = income[\"sex\"].replace({1: \"male\", 0: \"female\"})\n",
        "# Sex variable values\n",
        "income.sex.value_counts()"
      ],
      "metadata": {
        "id": "2lQmyLh6gGfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Target mapping\n",
        "income[\"target\"] = income[\"target\"].replace({1: \"disease\", 0: \"normal\"})\n",
        "# Target distribution\n",
        "income.target.value_counts()"
      ],
      "metadata": {
        "id": "bEtIimPbgpwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chest Pain values mapping\n",
        "income[\"cp\"] = income[\"cp\"].replace({4: np.nan, 3: \"asymptomatic\", 2: \"non_anginal_pain\", 1: \"atypical_angina\", 0: \"typical_angina\"})\n",
        "# Chest Pain distribution\n",
        "income.cp.value_counts()"
      ],
      "metadata": {
        "id": "osdB-WVohJth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert resting blood pressure\n",
        "income.trestbps = pd.to_numeric(income.trestbps, errors = 'coerce')\n",
        "# Convert cholesterol\n",
        "income.chol = pd.to_numeric(income.chol, errors = 'coerce')\n",
        "# Convert maximum heart rate\n",
        "income.thalachh = pd.to_numeric(income.thalachh, errors = 'coerce')"
      ],
      "metadata": {
        "id": "JqLciWCbhiW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Listagem de colunas categóricas"
      ],
      "metadata": {
        "id": "5MCg2ohUznDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list of categorical feature variables\n",
        "categorical = income.drop(columns = \"target\").select_dtypes(\"object\").columns.to_list()\n",
        "numerical = income.select_dtypes(\"number\").columns.to_list()\n",
        "\n",
        "# For each categorical variable\n",
        "for cat in categorical:\n",
        "    # Print the variable and its values\n",
        "    print(f\"{cat} --> {income[cat].unique()}\")"
      ],
      "metadata": {
        "id": "ZP29ysPRlcP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fasting blood sugar values mapping\n",
        "income[\"fbs\"] = income[\"fbs\"].replace({\"1\": \"high_fbs\", \"0\": \"low_fbs\",\n",
        "                               \"?\": np.nan})\n",
        "\n",
        "# Resting electrocardiographic results mapping\n",
        "income[\"restecg\"] = income[\"restecg\"].replace({\"2\": \"left_ventricular_hypertrophy\",\n",
        "                                       \"1\": \"st_t_wave_abnormality\",\n",
        "                                       \"0\": \"normal\", \"?\": np.nan})\n",
        "\n",
        "# Exercise-induced angina values mapping\n",
        "income[\"exang\"] = income[\"exang\"].replace({\"1\": \"yes\", \"0\": \"no\",\n",
        "                                   \"?\": np.nan})\n",
        "\n",
        "# Slope values mapping\n",
        "income[\"slope\"] = income[\"slope\"].replace({\"3\": np.nan, \"2\": \"downsloping\",\n",
        "                                   \"1\": \"flat\", \"0\": \"upsloping\", \"?\": np.nan})\n",
        "\n",
        "# Major vessels mapping\n",
        "income[\"ca\"] = income[\"ca\"].replace({\"4\": np.nan, \"3\": \"three_vessels\", \"2\": \"two_vessels\",\n",
        "                             \"1\": \"one_vessel\", \"0\": \"no_vessel\", \"?\": np.nan})\n",
        "\n",
        "# Thalassemia types values mapping\n",
        "income[\"thal\"] = income[\"thal\"].replace({\"7\": np.nan, \"6\": np.nan, \"3\": \"reversible_defect\",\n",
        "                                 \"2\": \"fixed_defect\", \"1\": \"normal\", \"0\": np.nan, \"?\": np.nan})"
      ],
      "metadata": {
        "id": "sEbed3ANlwaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For each categorical variable\n",
        "for cat in categorical:\n",
        "    # Print the variable and its values\n",
        "    print(f\"{cat} --> {income[cat].unique()}\")"
      ],
      "metadata": {
        "id": "XIzvbzSFmOBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Informações do Dataset após a realização das conversões."
      ],
      "metadata": {
        "id": "g3l4qvRc1vM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "income.info()"
      ],
      "metadata": {
        "id": "5r0Es96cioEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Envio para o wandb**\n",
        "\n",
        "O repositório criado no wanb irá armazenar os dados de treinamento e teste, além de métricas, parãmetros e gráficos importantes para o trabalho."
      ],
      "metadata": {
        "id": "ezk61C9hhjS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "income.to_csv(\"raw_data.csv\",index=False)"
      ],
      "metadata": {
        "id": "BAmvL54V5s4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to Weights & Biases\n",
        "!wandb login --relogin"
      ],
      "metadata": {
        "id": "VJeqUSHbHz4R",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Send the raw_data.csv to the Wandb storing it as an artifact\n",
        "!wandb artifact put \\\n",
        "      --name Heart_Disease/raw_data.csv \\\n",
        "      --type raw_data \\\n",
        "      --description \"The raw data from Heart Disease App\" raw_data.csv"
      ],
      "metadata": {
        "id": "LTMHC2JFI4Bw",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save_code tracking all changes of the notebook and sync with Wandb\n",
        "run = wandb.init(project=\"Heart_Disease\", save_code=True)\n",
        "\n",
        "# donwload the latest version of artifact raw_data.csv\n",
        "artifact = run.use_artifact(\"Heart_Disease/raw_data.csv:latest\")\n",
        "\n",
        "# create a dataframe from the artifact\n",
        "df = pd.read_csv(artifact.file())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "b813MbhZPOuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indicação de dados nulos em cada coluna do Dataset salvo no wandb"
      ],
      "metadata": {
        "id": "lUbpljA4z1QI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "HRhU4mUErNFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Realizada a EDA a partir da biblioteca Pandas Profiling**\n",
        "\n",
        "A análise exploratória de dados do projeto foi realizada através da biblioteca Pandas Profiling, que emite o relatório de cada feature do Dataset, além do contexto geral dos dados trabalhados."
      ],
      "metadata": {
        "id": "YFnYqwqKV7YB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ProfileReport(df, title=\"Pandas Profiling Report\", explorative=True)"
      ],
      "metadata": {
        "id": "QGRqE2LWTvUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preparação e Engenharia de Atributos**\n",
        "Realizar a limpeza (removendo ou imputando\n",
        "registros incompletos) e transformar atributos para\n",
        "um formato adequado ao modelo."
      ],
      "metadata": {
        "id": "1x768JnQ-pN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Correção de valores perdidos**\n",
        "\n",
        "O dataset trabalhado possui categorias com alto índice de dados perdidos, sendo assim será feita uma correção no dataset para contornar esse problema intriseco."
      ],
      "metadata": {
        "id": "Rj2IrHOYpBK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to Weights & Biases\n",
        "!wandb login --relogin"
      ],
      "metadata": {
        "id": "IfA9Qc-XxDlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save_code tracking all changes of the notebook and sync with Wandb\n",
        "run = wandb.init(project=\"Heart_Disease\", save_code=True)\n",
        "\n",
        "# donwload the latest version of artifact raw_data.csv\n",
        "artifact = run.use_artifact(\"Heart_Disease/raw_data.csv:latest\")\n",
        "\n",
        "# create a dataframe from the artifact\n",
        "df = pd.read_csv(artifact.file())"
      ],
      "metadata": {
        "id": "7no8zy73xEKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "3h_kZ2t5xmS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Percentages of missing values\n",
        "round(100 * df.isnull().sum() / len(df), 2)"
      ],
      "metadata": {
        "id": "2yQpAfWqpATB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As classes a seguir possuem uma baixa perda de valores, sendo assim a correção será feita aplicando a mediana para valores numéricos e a moda para valores categoricos."
      ],
      "metadata": {
        "id": "-bRLyeB2p8qP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replacing numerical missing values by median\n",
        "df.fillna(value = df[[\"trestbps\", \"chol\", \"thalachh\"]].median(), inplace = True)\n",
        "\n",
        "# Replacing categorical missing values by mode\n",
        "df[\"fbs\"] = df[\"fbs\"].fillna(value = df[\"fbs\"].mode()[0])\n",
        "df[\"restecg\"] = df[\"restecg\"].fillna(value = df[\"restecg\"].mode()[0])\n",
        "df[\"exang\"] = df[\"exang\"].fillna(value = df[\"exang\"].mode()[0])"
      ],
      "metadata": {
        "id": "4EI3zLvlpsPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algumas observações podem ter muitos valores ausentes em colunas diferentes. Não deve substituí-los diretamente, pois isso pode inserir vieses nos dados trabalhados, será feita a verificação das observações que possuem valores ausentes em pelo menos duas colunas e em seguida será feita a sua exclusão."
      ],
      "metadata": {
        "id": "PL3kU-l3qQ4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of columns with missing values\n",
        "missing_features = [\"cp\", \"slope\", \"ca\", \"thal\"]\n",
        "\n",
        "# For each variale\n",
        "for first_miss in missing_features:\n",
        "    # Check another variable\n",
        "    for second_miss in missing_features:\n",
        "        # Make sure those variables are different\n",
        "        if first_miss != second_miss:\n",
        "            # Get index of observations to drop\n",
        "            index_to_drop = (df[(df[first_miss].isnull())\n",
        "                             & (df[second_miss].isnull())].index)\n",
        "            # Drop missing data\n",
        "            df.drop(index_to_drop, axis = 'index', inplace = True)\n",
        "\n",
        "# Percentages of missing values\n",
        "round(100 * df.isnull().sum() / len(df), 2)"
      ],
      "metadata": {
        "id": "m05ILIjMp5Wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replacing categorical missing values by mode\n",
        "df[\"cp\"] = df[\"cp\"].fillna(value = df[\"cp\"].mode()[0])\n",
        "df[\"slope\"] = df[\"slope\"].fillna(value = df[\"slope\"].mode()[0])\n",
        "df[\"ca\"] = df[\"ca\"].fillna(value = df[\"ca\"].mode()[0])\n",
        "df[\"thal\"] = df[\"thal\"].fillna(value = df[\"thal\"].mode()[0])"
      ],
      "metadata": {
        "id": "ps3o0dwIrEhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count of missing values\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "gUK1t5lkrHV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Codificação da feature de classificação do problema (\"target\")."
      ],
      "metadata": {
        "id": "MphGqhQ13wti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Target encoding\n",
        "df[\"target\"] = (df[\"target\"] == \"disease\").astype(int)"
      ],
      "metadata": {
        "id": "ouDLkyfEuxZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "mDHx-cgNu0Jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Realização do Pre-Processing**"
      ],
      "metadata": {
        "id": "qFADI-kQvrKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login --relogin"
      ],
      "metadata": {
        "id": "d__eRYjG-r6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_artifact=\"Heart_Disease/raw_data.csv:latest\"\n",
        "artifact_name=\"preprocessed_data.csv\"\n",
        "artifact_type=\"clean_data\"\n",
        "artifact_description=\"Data after preprocessing\""
      ],
      "metadata": {
        "id": "aBspKx1rvy6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new job_type\n",
        "run = wandb.init(project=\"Heart_Disease\", job_type=\"process_data\")"
      ],
      "metadata": {
        "id": "2puv__XXxtz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# donwload the latest version of artifact raw_data.csv\n",
        "artifact = run.use_artifact(input_artifact)\n",
        "\n",
        "# create a dataframe from the artifact\n",
        "df = pd.read_csv(artifact.file())"
      ],
      "metadata": {
        "id": "2Izywly-x5Sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "7Kz3_xFBZY6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantitativo de repetições de dados catalogados."
      ],
      "metadata": {
        "id": "CnlBQ7qP3_Ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "nTNQkYg6aym3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete duplicated rows\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "df.info()"
      ],
      "metadata": {
        "id": "WeLnlsAhyLaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Envio do Dataset limpo para o wandb"
      ],
      "metadata": {
        "id": "01B1NxMn4VWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a \"clean data file\"\n",
        "df.to_csv(artifact_name,index=False)"
      ],
      "metadata": {
        "id": "mQ_CoLwpZQR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new artifact and configure with the necessary arguments\n",
        "artifact = wandb.Artifact(name=artifact_name,\n",
        "                          type=artifact_type,\n",
        "                          description=artifact_description)\n",
        "artifact.add_file(artifact_name)\n",
        "\n",
        "# Upload the artifact to Wandb\n",
        "run.log_artifact(artifact)"
      ],
      "metadata": {
        "id": "AMmU0R46yE1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# close the run\n",
        "# waiting a while after run the previous cell before execute this\n",
        "run.finish()"
      ],
      "metadata": {
        "id": "lFll4moMydaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Realização do Data Check com *pytest***"
      ],
      "metadata": {
        "id": "ye2P7ISR1zXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to Weights & Biases\n",
        "!wandb login --relogin"
      ],
      "metadata": {
        "id": "hhXxsHmmDzHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file test_data.py\n",
        "import pytest\n",
        "\n",
        "# This is global so all tests are collected under the same run\n",
        "run = wandb.init(project=\"Heart_Disease\", job_type=\"data_checks\")\n",
        "\n",
        "@pytest.fixture(scope=\"session\")\n",
        "def data():\n",
        "\n",
        "    local_path = run.use_artifact(\"Heart_Disease/preprocessed_data.csv:latest\").file()\n",
        "    df = pd.read_csv(local_path)\n",
        "\n",
        "    return df\n",
        "\n",
        "def test_data_length(data):\n",
        "    \"\"\"\n",
        "    We test that we have enough data to continue\n",
        "    \"\"\"\n",
        "    assert len(data) > 500\n",
        "\n",
        "\n",
        "def test_number_of_columns(data):\n",
        "    \"\"\"\n",
        "    We test that we have enough data to continue\n",
        "    \"\"\"\n",
        "    assert data.shape[1] == 14\n",
        "\n",
        "def test_column_presence_and_type(data):\n",
        "\n",
        "    required_columns = {\n",
        "        \"age\": pd.api.types.is_int64_dtype,\n",
        "        \"sex\": pd.api.types.is_object_dtype,\n",
        "        \"cp\": pd.api.types.is_object_dtype,\n",
        "        \"trestbps\": pd.api.types.is_float_dtype,\n",
        "        \"chol\": pd.api.types.is_float_dtype,\n",
        "        \"fbs\": pd.api.types.is_object_dtype,\n",
        "        \"restecg\": pd.api.types.is_object_dtype,\n",
        "        \"thalachh\": pd.api.types.is_float_dtype,\n",
        "        \"exang\": pd.api.types.is_object_dtype,\n",
        "        \"oldpeak\": pd.api.types.is_float_dtype,\n",
        "        \"slope\": pd.api.types.is_object_dtype,\n",
        "        \"ca\": pd.api.types.is_object_dtype,\n",
        "        \"thal\": pd.api.types.is_object_dtype,\n",
        "        \"target\": pd.api.types.is_int64_dtype\n",
        "    }\n",
        "\n",
        "    # Check column presence\n",
        "    assert set(data.columns.values).issuperset(set(required_columns.keys()))\n",
        "\n",
        "    for col_name, format_verification_funct in required_columns.items():\n",
        "\n",
        "        assert format_verification_funct(data[col_name]), f\"Column {col_name} failed test {format_verification_funct}\"\n",
        "\n",
        "\n",
        "def test_class_names(data):\n",
        "\n",
        "    # Check that only the known classes are present\n",
        "    known_classes = [\n",
        "        0,\n",
        "        1\n",
        "    ]\n",
        "\n",
        "    assert data[\"target\"].isin(known_classes).all()\n",
        "\n",
        "\n",
        "def test_column_ranges(data):\n",
        "\n",
        "    ranges = {\n",
        "        \"age\": (28, 77),\n",
        "        \"trestbps\": (92, 200),\n",
        "        \"chol\": (85, 603),\n",
        "        \"thalachh\": (71, 202),\n",
        "        \"oldpeak\": (0, 6.2)\n",
        "    }\n",
        "\n",
        "    for col_name, (minimum, maximum) in ranges.items():\n",
        "\n",
        "        assert data[col_name].dropna().between(minimum, maximum).all(), (\n",
        "            f\"Column {col_name} failed the test. Should be between {minimum} and {maximum}, \"\n",
        "            f\"instead min={data[col_name].min()} and max={data[col_name].max()}\"\n",
        "        )"
      ],
      "metadata": {
        "id": "guhWTnKK19pI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest . -vv"
      ],
      "metadata": {
        "id": "DAdAJR8TUBAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# close the run\n",
        "# waiting a while after run the previous cell before execute this\n",
        "run.finish()"
      ],
      "metadata": {
        "id": "_P1LiOhIWKlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Segragation**\n",
        "\n",
        "Separação de dados para treinamento e teste e envio para o wandb."
      ],
      "metadata": {
        "id": "Q8O6GuAs0WRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to Weights & Biases\n",
        "!wandb login --relogin"
      ],
      "metadata": {
        "id": "geLZ0T6O1Cr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# global variables\n",
        "\n",
        "# ratio used to split train and test data\n",
        "test_size = 0.20\n",
        "\n",
        "# seed used to reproduce purposes\n",
        "seed = 41\n",
        "\n",
        "# reference (column) to stratify the data\n",
        "stratify = \"target\"\n",
        "\n",
        "# name of the input artifact\n",
        "artifact_input_name = \"Heart_Disease/preprocessed_data.csv:latest\"\n",
        "\n",
        "# type of the artifact\n",
        "artifact_type = \"segregated_data\""
      ],
      "metadata": {
        "id": "e3qtXQMZ1H5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configure logging\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format=\"%(asctime)s %(message)s\",\n",
        "                    datefmt='%d-%m-%Y %H:%M:%S')\n",
        "\n",
        "# reference for a logging obj\n",
        "logger = logging.getLogger()\n",
        "\n",
        "# initiate wandb project\n",
        "run = wandb.init(project=\"Heart_Disease\", job_type=\"split_data\")\n",
        "\n",
        "logger.info(\"Downloading and reading artifact\")\n",
        "artifact = run.use_artifact(artifact_input_name)\n",
        "artifact_path = artifact.file()\n",
        "df = pd.read_csv(artifact_path)\n",
        "\n",
        "# Split firstly in train/test, then we further divide the dataset to train and validation\n",
        "logger.info(\"Splitting data into train and test\")\n",
        "splits = {}\n",
        "\n",
        "splits[\"train\"], splits[\"test\"] = train_test_split(df,\n",
        "                                                   test_size=test_size,\n",
        "                                                   random_state=seed,\n",
        "                                                   stratify=df[stratify])\n",
        "\n",
        "# Save the artifacts. We use a temporary directory so we do not leave any trace behind\n",
        "with tempfile.TemporaryDirectory() as tmp_dir:\n",
        "\n",
        "    for split, df in splits.items():\n",
        "\n",
        "        # Make the artifact name from the name of the split plus the provided root\n",
        "        artifact_name = f\"{split}.csv\"\n",
        "\n",
        "        # Get the path on disk within the temp directory\n",
        "        temp_path = os.path.join(tmp_dir, artifact_name)\n",
        "\n",
        "        logger.info(f\"Uploading the {split} dataset to {artifact_name}\")\n",
        "\n",
        "        # Save then upload to W&B\n",
        "        df.to_csv(temp_path,index=False)\n",
        "        artifact = wandb.Artifact(name=artifact_name,\n",
        "                                  type=artifact_type,\n",
        "                                  description=f\"{split} split of dataset {artifact_input_name}\",\n",
        "        )\n",
        "        artifact.add_file(temp_path)\n",
        "\n",
        "        logger.info(\"Logging artifact\")\n",
        "        run.log_artifact(artifact)\n",
        "\n",
        "        # This waits for the artifact to be uploaded to W&B. If you\n",
        "        # do not add this, the temp directory might be removed before\n",
        "        # W&B had a chance to upload the datasets, and the upload\n",
        "        # might fail\n",
        "        artifact.wait()"
      ],
      "metadata": {
        "id": "a9m_9E251gGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# close the run\n",
        "# waiting a while after run the previous cell before execute this\n",
        "run.finish()"
      ],
      "metadata": {
        "id": "8yX1cuss1hMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementação do Modelo em Pytorch**\n",
        "Utilizando a classe base fornecida pelo professor em\n",
        "aula, implementar a regressão logística."
      ],
      "metadata": {
        "id": "DGVLCneC-smJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "plt.style.use('fivethirtyeight')"
      ],
      "metadata": {
        "id": "4it2Qt-0_GGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Architecture(object):\n",
        "    def __init__(self, model, loss_fn, optimizer):\n",
        "        # Aqui definimos os atributos inicialização da nossa classe\n",
        "\n",
        "        # Começamos armazenando os argumentos como atributos\n",
        "        # para usá-los mais tarde\n",
        "        self.model = model #modelo\n",
        "        self.loss_fn = loss_fn #função de perda\n",
        "        self.optimizer = optimizer #Otimizador\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # Vamos enviar o modelo para o device ('cuda') especificado\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Esses atributos são definidos aqui, mas como não são\n",
        "        # informados no momento da criação, mantemos o estado 'None'\n",
        "        self.train_loader = None\n",
        "        self.val_loader = None\n",
        "\n",
        "        # Esses atributos serão computados internamente\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        self.total_epochs = 0\n",
        "\n",
        "        # Criação a função train_step para o nosso modelo,\n",
        "        # função de perda e otimizador\n",
        "        # Observação: NÃO HÁ ARGUMENTOS aqui! Ele usa a classe\n",
        "        # \"attributes\" diretamente\n",
        "        self.train_step_fn = self._make_train_step_fn()\n",
        "        # Cria a função val_step para nosso modelo e perda\n",
        "        self.val_step_fn = self._make_val_step_fn()\n",
        "\n",
        "    def to(self, device):\n",
        "        # Este método permite que o usuário especifique um device diferente\n",
        "        # Ele define o atributo correspondente (a ser usado posteriormente\n",
        "        # nos mini-batches) e envia o modelo para o device\n",
        "        try:\n",
        "            self.device = device\n",
        "            self.model.to(self.device)\n",
        "        except RuntimeError:\n",
        "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "            print(f\"Couldn't send it to {device}, sending it to {self.device} instead.\")\n",
        "            self.model.to(self.device)\n",
        "\n",
        "    def set_loaders(self, train_loader, val_loader=None):\n",
        "        # Este método permite que o usuário defina qual train_loader (e val_loader, opcionalmente) usar.\n",
        "        # Ambos os loaders são então atribuídos aos atributos da classe.\n",
        "        # Para que possam ser referenciados posteriormente.\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "\n",
        "    def _make_train_step_fn(self):\n",
        "        # Este método não precisa de argumentos... ele pode se referir aos\n",
        "        # atributos: self.model, self.loss_fn e self.optimizer\n",
        "\n",
        "        # Cria uma função que executa uma etapa no loop de treinamento\n",
        "        def perform_train_step_fn(x, y):\n",
        "            # Define o modelo para o modo de treinamento\n",
        "            self.model.train()\n",
        "\n",
        "            # Passo 1 - Computa a saída prevista do nosso modelo - forward pass\n",
        "            yhat = self.model(x)\n",
        "            # Passo 2 - Computa as perdas\n",
        "            loss = self.loss_fn(yhat, y)\n",
        "            # Passo 3 - Calcula os gradientes para os parâmetros \"a\" e \"b\"\n",
        "            loss.backward()\n",
        "            # Passo 4 - Atualiza os parâmetros usando os gradientes atualidados\n",
        "            # e a taxa de aprendizagem\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Retorna as perdas\n",
        "            return loss.item()\n",
        "\n",
        "        # Retorna a função que será chamada dentro do loop de treinamento\n",
        "        return perform_train_step_fn\n",
        "\n",
        "    def _make_val_step_fn(self):\n",
        "        # Cria uma função que executa uma etapa no loop de validação\n",
        "        def perform_val_step_fn(x, y):\n",
        "            # Define o modelo para o modo EVAL\n",
        "            self.model.eval()\n",
        "\n",
        "            # Passo 1\n",
        "            yhat = self.model(x)\n",
        "            # Passo 2\n",
        "            loss = self.loss_fn(yhat, y)\n",
        "            # Não há necessidade de calcular as etapas 3 e 4, pois não\n",
        "            # atualizamos os parâmetros durante a avaliação\n",
        "            return loss.item()\n",
        "\n",
        "        return perform_val_step_fn\n",
        "\n",
        "    def _mini_batch(self, validation=False):\n",
        "        # O mini-batch pode ser usado com ambos os loaders\n",
        "        # O argumento 'validation' define qual carregador e\n",
        "        # a função de step correspondente serão usados\n",
        "        if validation:\n",
        "            data_loader = self.val_loader\n",
        "            step_fn = self.val_step_fn\n",
        "        else:\n",
        "            data_loader = self.train_loader\n",
        "            step_fn = self.train_step_fn\n",
        "\n",
        "        if data_loader is None:\n",
        "            return None\n",
        "\n",
        "        # Uma vez que o data loader e a função de step, este é o mesmo\n",
        "        # loop de mini-batch que tínhamos antes\n",
        "        mini_batch_losses = []\n",
        "        for x_batch, y_batch in data_loader:\n",
        "            x_batch = x_batch.to(self.device)\n",
        "            y_batch = y_batch.to(self.device)\n",
        "\n",
        "            mini_batch_loss = step_fn(x_batch, y_batch)\n",
        "            mini_batch_losses.append(mini_batch_loss)\n",
        "\n",
        "        loss = np.mean(mini_batch_losses)\n",
        "        return loss\n",
        "\n",
        "    def set_seed(self, seed=42):\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    def train(self, n_epochs, seed=42):\n",
        "        # Para garantir a reprodutibilidade do processo de treinamento\n",
        "        self.set_seed(seed)\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            # Mantém o registro do número de épocas\n",
        "            # atualizando o atributo correspondente\n",
        "            self.total_epochs += 1\n",
        "\n",
        "            # loop interno\n",
        "            # Executa treinamento usando mini-batches\n",
        "            loss = self._mini_batch(validation=False)\n",
        "            self.losses.append(loss)\n",
        "\n",
        "            # VALIDAÇÃO\n",
        "            # SEM GRADIENTES NA VALIDAÇÃO!\n",
        "            with torch.no_grad():\n",
        "                # Executa avaliação usando mini-batches\n",
        "                val_loss = self._mini_batch(validation=True)\n",
        "                self.val_losses.append(val_loss)\n",
        "\n",
        "    def save_checkpoint(self, filename):\n",
        "        # Cria um dicionário com todos os elementos para retomar o treinamento\n",
        "        checkpoint = {'epoch': self.total_epochs,\n",
        "                      'model_state_dict': self.model.state_dict(),\n",
        "                      'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                      'loss': self.losses,\n",
        "                      'val_loss': self.val_losses}\n",
        "\n",
        "        torch.save(checkpoint, filename)\n",
        "\n",
        "    def load_checkpoint(self, filename):\n",
        "        # Carrega o dicionário\n",
        "        checkpoint = torch.load(filename,weights_only=False)\n",
        "\n",
        "        # Restaurar estado para modelo e otimizador\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        self.total_epochs = checkpoint['epoch']\n",
        "        self.losses = checkpoint['loss']\n",
        "        self.val_losses = checkpoint['val_loss']\n",
        "\n",
        "        self.model.train() # sempre use TRAIN para retomar o treinamento\n",
        "\n",
        "    def predict(self, x):\n",
        "        # O conjunto está no modo de avaliação para previsões\n",
        "        self.model.eval()\n",
        "        # Recebe uma entrada Numpy e a transforma em um tensor float\n",
        "        x_tensor = torch.as_tensor(x).float()\n",
        "        # Envia entrada para o device e usa o modelo para previsão\n",
        "        y_hat_tensor = self.model(x_tensor.to(self.device))\n",
        "        # Coloca de volta no modo de treinamento\n",
        "        self.model.train()\n",
        "        # Desconecta-o e leva-o para a CPU e de volta para o Numpy\n",
        "        return y_hat_tensor.detach().cpu().numpy()\n",
        "\n",
        "    def plot_losses(self):\n",
        "        fig = plt.figure(figsize=(10, 4))\n",
        "        plt.plot(self.losses, label='Training Loss', c='b')\n",
        "        plt.plot(self.val_losses, label='Validation Loss', c='r')\n",
        "        plt.yscale('log')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        return fig"
      ],
      "metadata": {
        "id": "nQqIrHmdslfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Treinamento do Modelo**\n",
        "Dividir o dataset em conjunto de treino e teste,\n",
        "configurar um otimizador, iterar por múltiplas\n",
        "épocas, calcular a loss e gradientes."
      ],
      "metadata": {
        "id": "8qIDgoKO-zD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to Weights & Biases\n",
        "!wandb login --relogin"
      ],
      "metadata": {
        "id": "Vuq35WVCoWkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Holdout Configuration\n",
        "\n",
        "Estratificação da feature de classificação do problema."
      ],
      "metadata": {
        "id": "Xoepxe3-lYls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# global variables\n",
        "\n",
        "# seed used to reproduce purposes\n",
        "seed = 41\n",
        "\n",
        "# reference (column) to stratify the data\n",
        "stratify = \"target\"\n",
        "\n",
        "# name of the input artifact\n",
        "artifact_input_name_train = \"Heart_Disease/train.csv:latest\"\n",
        "artifact_input_name_test = \"Heart_Disease/test.csv:latest\"\n",
        "\n",
        "# type of the artifact\n",
        "artifact_type_train = \"Train\"\n",
        "artifact_type_test = \"Test\""
      ],
      "metadata": {
        "id": "3QiPdxHNhFfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configure logging\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format=\"%(asctime)s %(message)s\",\n",
        "                    datefmt='%d-%m-%Y %H:%M:%S')\n",
        "\n",
        "# reference for a logging obj\n",
        "logger = logging.getLogger()\n",
        "\n",
        "# initiate the wandb project\n",
        "run = wandb.init(project=\"Heart_Disease\",job_type=\"train\")\n",
        "\n",
        "logger.warning(\"Downloading and reading train artifact\")\n",
        "local_path_train = run.use_artifact(artifact_input_name_train).file()\n",
        "df_train = pd.read_csv(local_path_train)\n",
        "\n",
        "logger.warning(\"Downloading and reading test artifact\")\n",
        "local_path_test = run.use_artifact(artifact_input_name_test).file()\n",
        "df_test = pd.read_csv(local_path_test)\n",
        "\n",
        "# Spliting train.csv into train and validation dataset\n",
        "logger.warning(\"Spliting data into train/val\")\n",
        "# split-out train/validation and test dataset\n",
        "X_train = df_train.drop(stratify, axis=1)\n",
        "y_train = df_train[stratify]\n",
        "X_test = df_test.drop(stratify, axis=1)\n",
        "y_test = df_test[stratify]"
      ],
      "metadata": {
        "id": "NV320NCDZcOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Codificando as colunas usando pipeline"
      ],
      "metadata": {
        "id": "bH_9A8Z8pqHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
        "    # Class Constructor\n",
        "    def __init__(self, feature_names):\n",
        "        self.feature_names = feature_names\n",
        "\n",
        "    # Return self nothing else to do here\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    # Method that describes what this custom transformer need to do\n",
        "    def transform(self, X, y=None):\n",
        "        return X[self.feature_names]"
      ],
      "metadata": {
        "id": "4megb9k4nSoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Categorical Features"
      ],
      "metadata": {
        "id": "gN3UaxXXs205"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling categorical features\n",
        "class CategoricalTransformer(BaseEstimator, TransformerMixin):\n",
        "    # Class constructor method that takes one boolean as its argument\n",
        "    def __init__(self, new_features=True, colnames=None):\n",
        "        self.new_features = new_features\n",
        "        self.colnames = colnames\n",
        "\n",
        "    # Return self nothing else to do here\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def get_feature_names_out(self):\n",
        "        return self.colnames.tolist()\n",
        "\n",
        "    # Transformer method we wrote for this transformer\n",
        "    def transform(self, X, y=None):\n",
        "        df = pd.DataFrame(X, columns=self.colnames)\n",
        "\n",
        "        # Remove white space in categorical features\n",
        "        df = df.apply(lambda row: row.str.strip())\n",
        "\n",
        "        # update column names\n",
        "        self.colnames = df.columns\n",
        "\n",
        "        return df\n"
      ],
      "metadata": {
        "id": "X3SKMO-_seMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Numerical Features"
      ],
      "metadata": {
        "id": "h7RZA9wdumw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transform numerical features\n",
        "class NumericalTransformer(BaseEstimator, TransformerMixin):\n",
        "    # Class constructor method that takes a model parameter as its argument\n",
        "    # model 0: minmax\n",
        "    # model 1: standard\n",
        "    # model 2: without scaler\n",
        "    def __init__(self, model=0, colnames=None):\n",
        "        self.model = model\n",
        "        self.colnames = colnames\n",
        "        self.scaler = None\n",
        "\n",
        "    # Fit is used only to learn statistical about Scalers\n",
        "    def fit(self, X, y=None):\n",
        "        df = pd.DataFrame(X, columns=self.colnames)\n",
        "        # minmax\n",
        "        if self.model == 0:\n",
        "            self.scaler = MinMaxScaler()\n",
        "            self.scaler.fit(df)\n",
        "        # standard scaler\n",
        "        elif self.model == 1:\n",
        "            self.scaler = StandardScaler()\n",
        "            self.scaler.fit(df)\n",
        "        return self\n",
        "\n",
        "    # return columns names after transformation\n",
        "    def get_feature_names_out(self):\n",
        "        return self.colnames\n",
        "\n",
        "    # Transformer method we wrote for this transformer\n",
        "    # Use fitted scalers\n",
        "    def transform(self, X, y=None):\n",
        "        df = pd.DataFrame(X, columns=self.colnames)\n",
        "\n",
        "        # update columns name\n",
        "        self.colnames = df.columns.tolist()\n",
        "\n",
        "        # minmax\n",
        "        if self.model == 0:\n",
        "            # transform data\n",
        "            df = self.scaler.transform(df)\n",
        "        elif self.model == 1:\n",
        "            # transform data\n",
        "            df = self.scaler.transform(df)\n",
        "        else:\n",
        "            df = df.values\n",
        "\n",
        "        return df"
      ],
      "metadata": {
        "id": "x5g43VstumYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation Pipeline\n",
        "\n",
        "Pipeline feito para aplicar as colunas categóricas a OneHotEnconder para converter em colunas numéricas. Além da normalização das colunas numéricas."
      ],
      "metadata": {
        "id": "OTrobhOOxLzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = 0 (min-max), 1 (z-score), 2 (without normalization)\n",
        "numerical_model = 0\n",
        "\n",
        "# Categrical features to pass down the categorical pipeline\n",
        "categorical_features = X_train.select_dtypes(\"object\").columns.to_list()\n",
        "\n",
        "# Numerical features to pass down the numerical pipeline\n",
        "numerical_features = X_train.select_dtypes('number').columns.to_list()\n",
        "\n",
        "# Defining the steps for the categorical pipeline\n",
        "categorical_pipeline = Pipeline(steps=[('cat_selector', FeatureSelector(categorical_features)),\n",
        "                                       ('imputer_cat', SimpleImputer(strategy=\"most_frequent\")),\n",
        "                                       ('cat_transformer', CategoricalTransformer(colnames=categorical_features)),\n",
        "                                       # ('cat_encoder','passthrough'\n",
        "                                       ('cat_encoder', OneHotEncoder(sparse_output=False, drop=\"first\"))\n",
        "                                       ]\n",
        "                                )\n",
        "\n",
        "# Defining the steps in the numerical pipeline\n",
        "numerical_pipeline = Pipeline(steps=[('num_selector', FeatureSelector(numerical_features)),\n",
        "                                     ('imputer_num', SimpleImputer(strategy=\"median\")),\n",
        "                                     ('num_transformer', NumericalTransformer(numerical_model,\n",
        "                                                                              colnames=numerical_features))])\n",
        "\n",
        "# Combine numerical and categorical pieplines into one full big pipeline horizontally\n",
        "full_pipeline_preprocessing = FeatureUnion(transformer_list=[('cat_pipeline', categorical_pipeline),\n",
        "                                                             ('num_pipeline', numerical_pipeline)]\n",
        "                                           )"
      ],
      "metadata": {
        "id": "X-6a6bbBxLZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "new_data_train = full_pipeline_preprocessing.fit_transform(X_train)\n",
        "# cat_names is a numpy array\n",
        "cat_names = full_pipeline_preprocessing.get_params()[\"cat_pipeline\"][3].get_feature_names_out().tolist()\n",
        "# num_names is a list\n",
        "num_names = full_pipeline_preprocessing.get_params()[\"num_pipeline\"][2].get_feature_names_out()\n",
        "X_train_normalized = pd.DataFrame(new_data_train,columns = cat_names + num_names)\n",
        "X_train_normalized.head()"
      ],
      "metadata": {
        "id": "3ojzDGBaxSJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "new_data_test = full_pipeline_preprocessing.fit_transform(X_test)\n",
        "# cat_names is a numpy array\n",
        "cat_names = full_pipeline_preprocessing.get_params()[\"cat_pipeline\"][3].get_feature_names_out().tolist()\n",
        "# num_names is a list\n",
        "num_names = full_pipeline_preprocessing.get_params()[\"num_pipeline\"][2].get_feature_names_out()\n",
        "X_test_normalized = pd.DataFrame(new_data_test,columns = cat_names + num_names)\n",
        "X_test_normalized.head()"
      ],
      "metadata": {
        "id": "WOkjfewrp3aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinamento utilizando o modelo desenvolvido no PyTorch"
      ],
      "metadata": {
        "id": "wnCMEPXz2BME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constantes utilizadas para o treio\n",
        "\n",
        "DATASET_FOLDER_PATH = \"dataset\" #caminho para o dataset\n",
        "BATCH_SIZE = 64 #número de batches para o treino\n",
        "LEARNING_RATE = 0.01 #taxa de aprendizado\n",
        "NUM_EPOCHS = 40 #Número de época para treinamento\n",
        "\n",
        "# Definição da função sigmoide para usar na avaliação\n",
        "def sigmoid(z):\n",
        " return 1 / (1 + np.exp(-z))\n",
        "\n",
        "\n",
        "# Configuração do dispositivo no pytorch para rodar na GPU se disponível\n",
        "# Caso não tenha GPU, o código rodará na CPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Dispositivo utilizado: {device}\")\n"
      ],
      "metadata": {
        "id": "Zu0WDyG93mtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparação dos dados, dada a necessidade de converter as colunas do dataset em valores numéricos"
      ],
      "metadata": {
        "id": "JLMN-oVG3bnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converter a coluna time_bin (categoria com intervalos) para valores numéricos\n",
        "if 'time_bin' in X_train_normalized.columns:\n",
        "    # Extrair o ponto médio de cada intervalo\n",
        "    X_train_normalized['time_bin'] = X_train_normalized['time_bin'].apply(lambda x: x.mid if isinstance(x, pd.Interval) else x)\n",
        "    X_test_normalized['time_bin'] = X_test_normalized['time_bin'].apply(lambda x: x.mid if isinstance(x, pd.Interval) else x)\n",
        "\n",
        "    print(\"Coluna time_bin convertida para valores numéricos (ponto médio do intervalo)\")\n",
        "\n",
        "# Agora é possível aplicar o StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_normalized)\n",
        "X_test_scaled = scaler.transform(X_test_normalized)\n",
        "\n",
        "print(\"Normalização concluída com sucesso!\")"
      ],
      "metadata": {
        "id": "PD569DBU4xoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conversão das entradas para tensores pytorch e criação dos loaders para treinamento."
      ],
      "metadata": {
        "id": "HWnU64Uz7DRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(13)\n",
        "\n",
        "# Constroi tensores a partir de arrays Numpy\n",
        "x_train_tensor = torch.FloatTensor(X_train_scaled)\n",
        "y_train_tensor = torch.FloatTensor(y_train.values.reshape(-1, 1))\n",
        "\n",
        "x_test_tensor = torch.FloatTensor(X_test_scaled)\n",
        "y_test_tensor = torch.FloatTensor(y_test.values.reshape(-1, 1))\n",
        "\n",
        "\n",
        "# Cria um conjunto de dados contendo TODOS os data points\n",
        "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
        "\n",
        "# Constroi um Loader de cada conjunto\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True\n",
        ")\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=16)"
      ],
      "metadata": {
        "id": "-dEaKZ9t3auD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir modelo, perda e otimizador\n",
        "input_dim = X_train_normalized.shape[1] # Número de features\n",
        "\n",
        "# Definir o modelo de regressão logística (uma única camada linear)\n",
        "torch.manual_seed(42)\n",
        "model = nn.Sequential()\n",
        "model.add_module('linear', nn.Linear(input_dim, 1))\n",
        "\n",
        "# Calcular peso para a classe positiva (Doente) para lidar com desbalanceamento\n",
        "n_samples = len(y_train)\n",
        "n_disease = np.sum(y_train)\n",
        "n_normal = n_samples - n_disease\n",
        "print(f\"\\nCalculando peso para classe positiva (doente):\")\n",
        "print(f\"Total de amostras: {n_samples}\")\n",
        "print(f\"Amostras normais: {n_normal}\")\n",
        "print(f\"Amostras doentes: {n_disease}\")\n",
        "print(f\"Proporção: {n_normal / n_disease:.2f}:1\")\n",
        "\n",
        "# Definir função de perda\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "# Definir otimizador SGD (Stochastic Gradient Descent)\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "qBZe9I0X1_39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar a arquitetura de treinamento\n",
        "arch = Architecture(model, loss_fn, optimizer)\n",
        "arch.set_loaders(train_loader, test_loader)\n",
        "# Treinar o modelo\n",
        "print(f\"Iniciando treinamento por {NUM_EPOCHS} épocas...\")\n",
        "arch.train(NUM_EPOCHS)\n",
        "print(\"Treinamento concluído!\")"
      ],
      "metadata": {
        "id": "nBP62jPJ4d0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotar evolução da perda\n",
        "loss_fig = arch.plot_losses()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eJ7ovB8u3UhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run.finish()"
      ],
      "metadata": {
        "id": "gWWeo_ynDP-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Avaliação do Modelo**\n",
        "Após treinado, avaliar o desempenho do classificador\n",
        "no conjunto de teste."
      ],
      "metadata": {
        "id": "RZHlBnW7-1iN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementações das métricas de avaliação"
      ],
      "metadata": {
        "id": "KtWPPJK7kwo5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\Large \\text{Recall} = \\frac{\\text{TP}}{\\text{TP + FN}} \\ \\ \\  \\text{Precision} = \\frac{\\text{TP}}{\\text{TP + FP}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\Large \\text{TPR} = \\frac{\\text{TP}}{\\text{TP + FN}} \\ \\ \\  \\text{FPR} = \\frac{\\text{FP}}{\\text{FP + TN}}\n",
        "$$"
      ],
      "metadata": {
        "id": "wpQGGMTOl0oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Métricas de avaliação do modelo\n",
        "def precision_recall(cm):\n",
        "    tn, fp = cm[0]\n",
        "    fn, tp = cm[1]\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    return precision, recall\n",
        "\n",
        "def tpr_fpr(cm):\n",
        "    tn, fp = cm[0]\n",
        "    fn, tp = cm[1]\n",
        "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "    return tpr, fpr\n",
        "\n",
        "# Cálculo de F1-score e especificidade\n",
        "def f1_score_manual(precision, recall):\n",
        "    return 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "def specificity(cm):\n",
        "    tn, fp = cm[0]\n",
        "    fn, tp = cm[1]\n",
        "    return tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "# Adicionando função para visualizar as probabilidades em uma linha\n",
        "def figure10(y, probabilities, threshold=0.5, shift=0.04, annot=True):\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 2))\n",
        "\n",
        "    # Configurações do plot\n",
        "    ax.grid(False)\n",
        "    ax.set_ylim([-.1, .1])\n",
        "    ax.axes.get_yaxis().set_visible(False)\n",
        "    ax.plot([0, 1], [0, 0], linewidth=2, c='k', zorder=1)\n",
        "    ax.plot([0, 0], [-.1, .1], c='k', zorder=1)\n",
        "    ax.plot([1, 1], [-.1, .1], c='k', zorder=1)\n",
        "\n",
        "    # Identificar os diferentes tipos de classificação\n",
        "    tn = (y == 0) & (probabilities < threshold)\n",
        "    fn = (y == 0) & (probabilities >= threshold)\n",
        "    tp = (y == 1) & (probabilities >= threshold)\n",
        "    fp = (y == 1) & (probabilities < threshold)\n",
        "\n",
        "    # Desenhar o threshold\n",
        "    ax.plot([threshold, threshold], [-.1, .1], c='k', zorder=1, linestyle='--')\n",
        "\n",
        "    # Desenhar os pontos\n",
        "    colors = ['#FF0000', '#0000FF']  # Vermelho e azul\n",
        "    ax.scatter(probabilities[tn], np.zeros(tn.sum()) + shift, c=colors[0], s=150, zorder=2, edgecolor=colors[0], linewidth=3)\n",
        "    ax.scatter(probabilities[fn], np.zeros(fn.sum()) + shift, c=colors[0], s=150, zorder=2, edgecolor=colors[1], linewidth=3)\n",
        "    ax.scatter(probabilities[tp], np.zeros(tp.sum()) - shift, c=colors[1], s=150, zorder=2, edgecolor=colors[1], linewidth=3)\n",
        "    ax.scatter(probabilities[fp], np.zeros(fp.sum()) - shift, c=colors[1], s=150, zorder=2, edgecolor=colors[0], linewidth=3)\n",
        "\n",
        "    ax.set_xlabel(r'$\\sigma(z) = P(y=1)$')\n",
        "    ax.set_title(f'Threshold = {threshold}')\n",
        "\n",
        "    if annot:\n",
        "        ax.annotate('TN', xy=(.20, .03), c='k', weight='bold', fontsize=20)\n",
        "        ax.annotate('FN', xy=(.20, -.08), c='k', weight='bold', fontsize=20)\n",
        "        ax.annotate('FP', xy=(.70, .03), c='k', weight='bold', fontsize=20)\n",
        "        ax.annotate('TP', xy=(.70, -.08), c='k', weight='bold', fontsize=20)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    return fig"
      ],
      "metadata": {
        "id": "SB4c4I4v-1MA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#exemplo de predição para validação\n",
        "logits_val = arch.predict(X_test_scaled[:5])\n",
        "logits_val\n",
        "\n",
        "# prediction probabilities\n",
        "prob_val = torch.sigmoid(torch.as_tensor(logits_val[:5]).float())\n",
        "prob_val"
      ],
      "metadata": {
        "id": "NNrU-c5SZS0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtendo as predições do modelo nos dados de teste\n",
        "logits = arch.predict(X_test_scaled)\n",
        "\n",
        "# Convertendo os logits para probabilidades usando a função sigmoid\n",
        "y_pred_proba = sigmoid(logits).squeeze()\n",
        "\n",
        "# Calculando a matriz de confusão com threshold padrão de 0.5\n",
        "cm_thresh50 = confusion_matrix(y_test, (y_pred_proba >= 0.5))\n",
        "\n",
        "# Plotar a matriz de confusão\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_thresh50, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Normal\", \"Doente\"], yticklabels=[\"Normal\", \"Doente\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Matriz de confusão (Threshold = 0.5)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "08oTbNJMWZhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraíndo as métricas\n",
        "precision, recall = precision_recall(cm_thresh50)\n",
        "print(f\"Precisão: {precision:.4f} \\nRecall: {recall:.4f}\")\n",
        "\n",
        "# Calcular acurácia\n",
        "accuracy = accuracy_score(y_test, (y_pred_proba >= 0.5))\n",
        "print(f\"Acurácia: {accuracy:.4f}\")\n",
        "\n",
        "# Calcular TPR e FPR\n",
        "tpr, fpr = tpr_fpr(cm_thresh50)\n",
        "print(f\"TPR: {tpr:.4f}\\nFPR: {fpr:.4f}\")\n",
        "\n",
        "# Calcular AUC-ROC\n",
        "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"AUC-ROC: {auc_score:.4f}\")"
      ],
      "metadata": {
        "id": "dxprZN5klCGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotagem da linha de probabilidades"
      ],
      "metadata": {
        "id": "mYLZU9k-jxVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizar a linha de probabilidades\n",
        "fig = figure10(y_test, y_pred_proba, threshold=0.5, shift=0.04, annot=True)\n",
        "plt.title('Distribuição de Probabilidades com Threshold = 0.5')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PCIcVOHQXqhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular F1-score e especificidade\n",
        "f1_score = f1_score_manual(precision, recall)\n",
        "specif = specificity(cm_thresh50)\n",
        "\n",
        "print(f\"F1-Score: {f1_score:.4f}\")\n",
        "print(f\"Especificidade: {specif:.4f}\")"
      ],
      "metadata": {
        "id": "ZyxHcPk9afgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizar curvas ROC e Precision-Recall\n",
        "plt.figure(figsize=(16, 6))\n",
        "\n",
        "# Curva ROC\n",
        "plt.subplot(1, 2, 1)\n",
        "fpr_curve, tpr_curve, thresholds_roc = roc_curve(y_test, y_pred_proba)\n",
        "plt.plot(fpr_curve, tpr_curve, lw=2, label=f'AUC = {auc_score:.4f}')\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Taxa de Falsos Positivos (FPR)')\n",
        "plt.ylabel('Taxa de Verdadeiros Positivos (TPR)')\n",
        "plt.title('Curva ROC')\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "# Curva Precision-Recall\n",
        "plt.subplot(1, 2, 2)\n",
        "precision_curve, recall_curve, thresholds_pr = precision_recall_curve(y_test, y_pred_proba)\n",
        "pr_auc = auc(recall_curve, precision_curve)\n",
        "plt.plot(recall_curve, precision_curve, lw=2, label=f'AUC = {pr_auc:.4f}')\n",
        "plt.plot([0, 1], [np.sum(y_test)/len(y_test)] * 2, 'k--', lw=2, label='Linha base')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Curva Precision-Recall')\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yzq0FW3GapcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Threshold ótimo\n",
        "\n",
        "Caso precise da obtenção em casos de desbalanceio. Como o Dataset trabalhado foi constatado como equilibrado na etapa de EDA, a aplicação dessa correção nele irá resultar num Threshold próximo a 0,5.\n"
      ],
      "metadata": {
        "id": "Ctqa91kfnCwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encontrar o threshold ótimo para o modelo desenvolvido\n",
        "def find_optimal_threshold(y_true, y_proba, metric='f1', thresholds=None):\n",
        "    if thresholds is None:\n",
        "        thresholds = np.arange(0.1, 0.9, 0.02)\n",
        "\n",
        "    best_metric = 0\n",
        "    best_threshold = 0.5\n",
        "    best_metrics = {}\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        y_pred = (y_proba >= threshold).astype(int)\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "        prec, rec = precision_recall(cm)\n",
        "        f1 = f1_score_manual(prec, rec)\n",
        "        tpr, fpr = tpr_fpr(cm)\n",
        "        spec = specificity(cm)\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "        # Escolher métrica para otimização\n",
        "        if metric == 'f1':\n",
        "            current_metric = f1\n",
        "        elif metric == 'recall':\n",
        "            current_metric = rec\n",
        "        elif metric == 'precision':\n",
        "            current_metric = prec\n",
        "        else:\n",
        "            # Usar uma métrica balanceada (média geométrica de TPR e TNR)\n",
        "            current_metric = np.sqrt(tpr * spec)\n",
        "\n",
        "        if current_metric > best_metric:\n",
        "            best_metric = current_metric\n",
        "            best_threshold = threshold\n",
        "            best_metrics = {\n",
        "                'threshold': threshold,\n",
        "                'accuracy': acc,\n",
        "                'precision': prec,\n",
        "                'recall': rec,\n",
        "                'specificity': spec,\n",
        "                'f1_score': f1,\n",
        "                'tpr': tpr,\n",
        "                'fpr': fpr\n",
        "            }\n",
        "\n",
        "    return best_threshold, best_metrics\n",
        "\n",
        "# Encontrar threshold ótimo e mostrar resultados\n",
        "print(\"\\nProcurando threshold ótimo...\")\n",
        "best_threshold, best_metrics = find_optimal_threshold(\n",
        "    y_test, y_pred_proba, metric='f1',\n",
        "    thresholds=np.arange(0.1, 0.9, 0.01)\n",
        ")\n"
      ],
      "metadata": {
        "id": "Rwb3cj9fbtzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizar como as métricas variam com o threshold\n",
        "thresholds = np.arange(0.05, 0.95, 0.05)\n",
        "metrics_by_threshold = {\n",
        "    'threshold': [],\n",
        "    'accuracy': [],\n",
        "    'precision': [],\n",
        "    'recall': [],\n",
        "    'specificity': [],\n",
        "    'f1_score': []\n",
        "}\n",
        "\n",
        "for threshold in thresholds:\n",
        "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    prec, rec = precision_recall(cm)\n",
        "    f1 = f1_score_manual(prec, rec)\n",
        "    spec = specificity(cm)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    metrics_by_threshold['threshold'].append(threshold)\n",
        "    metrics_by_threshold['accuracy'].append(acc)\n",
        "    metrics_by_threshold['precision'].append(prec)\n",
        "    metrics_by_threshold['recall'].append(rec)\n",
        "    metrics_by_threshold['specificity'].append(spec)\n",
        "    metrics_by_threshold['f1_score'].append(f1)\n",
        "\n",
        "# Plotar as métricas por threshold\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(metrics_by_threshold['threshold'], metrics_by_threshold['precision'], label='Precision')\n",
        "plt.plot(metrics_by_threshold['threshold'], metrics_by_threshold['recall'], label='Recall')\n",
        "plt.plot(metrics_by_threshold['threshold'], metrics_by_threshold['f1_score'], label='F1-Score')\n",
        "plt.plot(metrics_by_threshold['threshold'], metrics_by_threshold['accuracy'], label='Acurácia')\n",
        "plt.plot(metrics_by_threshold['threshold'], metrics_by_threshold['specificity'], label='Especificidade')\n",
        "plt.axvline(x=best_threshold, color='k', linestyle='--', label=f'Threshold Ótimo ({best_threshold:.2f})')\n",
        "\n",
        "plt.xlabel('Threshold')\n",
        "plt.ylabel('Valor da Métrica')\n",
        "plt.title('Métricas por Threshold')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jzpVRiyibyDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exportação do melhor modelo para o wandb"
      ],
      "metadata": {
        "id": "NCvAIWKtjy_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to Weights & Biases\n",
        "!wandb login --relogin"
      ],
      "metadata": {
        "id": "y1Ch1lRjkQ9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run = wandb.init(project=\"Heart_Disease\",job_type=\"train\")"
      ],
      "metadata": {
        "id": "IXMwpS0VrQng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training\n",
        "logger.warning(\"Training\")\n",
        "arch.train(NUM_EPOCHS)\n",
        "\n",
        "# predict\n",
        "logger.warning(\"Infering\")\n",
        "predict = arch.predict(X_test_scaled)\n",
        "\n",
        "# Evaluation Metrics\n",
        "logger.warning(\"Evaluation metrics\")\n",
        "fbeta = best_metrics['f1_score']\n",
        "precision = best_metrics['precision']\n",
        "recall = best_metrics['recall']\n",
        "acc = best_metrics['accuracy']\n",
        "\n",
        "logger.warning(\"Accuracy: {}\".format(acc))\n",
        "logger.warning(\"Precision: {}\".format(precision))\n",
        "logger.warning(\"Recall: {}\".format(recall))\n",
        "logger.warning(\"F1: {}\".format(fbeta))\n",
        "\n",
        "run.summary[\"Acc\"] = best_metrics['accuracy']\n",
        "run.summary[\"Precision\"] = best_metrics['precision']\n",
        "run.summary[\"Recall\"] = best_metrics['recall']\n",
        "run.summary[\"F1\"] = best_metrics['f1_score']\n"
      ],
      "metadata": {
        "id": "_GMrRpGYlEPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run.finish()"
      ],
      "metadata": {
        "id": "sm59UTm8r5X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Documentação dos Resultados**\n",
        "Apresentar de forma clara os resultados obtidos,\n",
        "incluindo as métricas calculadas, alguma discussão\n",
        "sobre o que elas indicam, e possivelmente gráficos\n",
        "ou tabelas que ilustram o desempenho."
      ],
      "metadata": {
        "id": "yViy-Say-33b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exibir os parâmetros do modelo treinado\n",
        "\n",
        "# Mostrar a equação de regressão logística resultante\n",
        "# Obter os pesos (coeficientes) e o viés (intercepto)\n",
        "if hasattr(model, 'linear'):\n",
        "    weights = model.linear.weight.data.cpu().numpy()[0]\n",
        "    bias = model.linear.bias.data.cpu().numpy()[0]\n",
        "\n",
        "    print(\"\\nEquação da Regressão Logística:\")\n",
        "    equation = f\"z = {bias:.4f}\"\n",
        "\n",
        "    for i, w in enumerate(weights):\n",
        "        sign = \"+\" if w >= 0 else \"\"\n",
        "        equation += f\" {sign} {w:.4f}*x{i+1}\"\n",
        "\n",
        "    print(equation)\n",
        "    print(\"\\nProbabilidade de fraude = 1 / (1 + exp(-z))\")\n",
        "\n",
        "n_disease_total = df_train['target'].sum()+df_test['target'].sum()\n",
        "n_samples_total = len(df_train)+len(df_test)\n",
        "\n",
        "# Resumo dos resultados\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"RESUMO DOS RESULTADOS - PROPENSÃO A DOENÇA CARDÍACA\")\n",
        "print(\"=\"*50)\n",
        "print(\"Modelo: Regressão Logística Binária\")\n",
        "print(f\"Número de features: {input_dim}\")\n",
        "print(f\"Total de amostras: {n_samples_total}\")\n",
        "print(f\"Amostras de treino: {len(y_train)}, Amostras de teste: {len(y_test)}\")\n",
        "print(f\"Proporção de propensos: {n_disease_total/n_samples_total*100:.2f}%\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Métricas com threshold padrão ({0.5}):\")\n",
        "print(f\"Acurácia: {accuracy:.4f}\")\n",
        "print(f\"Precisão: {precision:.4f}\")\n",
        "print(f\"Recall (Sensitividade): {recall:.4f}\")\n",
        "print(f\"Especificidade: {specif:.4f}\")\n",
        "print(f\"F1-Score: {f1_score:.4f}\")\n",
        "print(f\"AUC-ROC: {auc_score:.4f}\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Métricas com threshold ótimo ({best_threshold:.2f}):\")\n",
        "print(f\"Acurácia: {best_metrics['accuracy']:.4f}\")\n",
        "print(f\"Precisão: {best_metrics['precision']:.4f}\")\n",
        "print(f\"Recall (Sensitividade): {best_metrics['recall']:.4f}\")\n",
        "print(f\"Especificidade: {best_metrics['specificity']:.4f}\")\n",
        "print(f\"F1-Score: {best_metrics['f1_score']:.4f}\")\n",
        "print(f\"AUC-ROC: {auc_score:.4f}\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "\n",
        "# Salvar o modelo e o scaler para uso futuro\n",
        "torch.save(model.state_dict(), \"modelo_regressao_logistica.pth\")"
      ],
      "metadata": {
        "id": "AlPpxt2O-3pq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}